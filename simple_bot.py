import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()

# åˆå§‹åŒ–ä¸¤ä¸ª LLM (æŒ‰èƒ½åŠ›åˆ†çº§)
api_key = os.getenv("OPENAI_API_KEY")
api_base = os.getenv("OPENAI_API_BASE")

fast_model_name = os.getenv("LLM_FAST_MODEL")
reasoning_model_name = os.getenv("LLM_REASONING_MODEL")

print(f"âš¡ï¸ Init Fast LLM: {fast_model_name}")
llm_fast = ChatOpenAI(
    model=fast_model_name,
    openai_api_key=api_key,
    openai_api_base=api_base,
    temperature=0.1 # Router éœ€è¦ç²¾å‡†
)

print(f"ğŸ§  Init Reasoning LLM: {reasoning_model_name}")
llm_reasoning = ChatOpenAI(
    model=reasoning_model_name,
    openai_api_key=api_key,
    openai_api_base=api_base,
    temperature=0.7 # Writer éœ€è¦åˆ›æ„
)

def get_bot_response(user_input: str) -> str:
    """
    æ ¸å¿ƒå‡½æ•°ï¼šæ¥æ”¶ç”¨æˆ·æ–‡æœ¬ -> è°ƒç”¨å¤§æ¨¡å‹ -> è¿”å›å›å¤
    """
    try:
        # Note: The original function used 'llm'.
        # You might need to update this to use 'router_llm' or 'writer_llm'
        # depending on your application logic.
        response = router_llm.invoke(user_input) # Changed to router_llm for demonstration
        return response.content
    except Exception as e:
        return f"Sorry, AI brain error: {str(e)}"

def test_bot():
    print("ğŸ¤– Sending request to:", llm.model_name)
    print("âœ… Response:", get_bot_response("Hello!"))

if __name__ == "__main__":
    test_bot()
