from typing import TypedDict, List, Optional, Dict
from langchain_core.messages import BaseMessage
from pydantic import BaseModel, Field
from typing import Literal

# --- Pydantic Data Models (ç”¨äº Writer ç»“æ„åŒ–è¾“å‡º) ---
class NewsItem(BaseModel):
    title: str = Field(..., description="æ–°é—»æ ‡é¢˜")
    summary: str = Field(..., description="æ–°é—»æ‘˜è¦")
    url: str = Field(..., description="åŸæ–‡é“¾æ¥")
    score: int = Field(..., description="é‡è¦æ€§æ‰“åˆ† 1-100")

class NewsCluster(BaseModel):
    name: str = Field(..., description="æ¿å—åç§°ï¼Œå¦‚'ç¡¬ä»¶ä¸ç®—åŠ›'")
    description: str = Field(..., description="æ¿å—ç»¼è¿°")
    items: List[NewsItem] = Field(..., description="è¯¥æ¿å—ä¸‹çš„æ–°é—»åˆ—è¡¨")

class NewsBriefing(BaseModel):
    global_summary: str = Field(..., description="å…¨ç¯‡æ—©æŠ¥çš„å¼€åœºç»¼è¿°")
    top_story_indices: List[int] = Field(None, description="ä»Šæ—¥å¤´æ¡æ–°é—»åœ¨ clusters ä¸­çš„ç´¢å¼•(æš‚ä¸ä½¿ç”¨)")
    # æ³¨æ„ï¼šä¸ºäº†ç®€åŒ–ï¼ŒTop 5 å¯ä»¥åœ¨å±•ç¤ºå±‚é€»è¾‘å¤„ç†ï¼Œæˆ–è€…ç›´æ¥å– clusters é‡Œ score æœ€é«˜çš„
    clusters: List[NewsCluster] = Field(..., description="æ–°é—»åˆ†ç±»æ¿å—")

# --- Agent State ---
class AgentState(TypedDict):
    # æ¶ˆæ¯å†å²
    messages: List[BaseMessage]
    user_id: str
    message_id: Optional[str]
    user_preference: Optional[str]
    news_content: Optional[str] 
    
    # [æ–°å¢] ç»“æ„åŒ–ç®€æŠ¥æ•°æ® (ç”¨äºå¤šè½®å›å¿†)
    briefing_data: Optional[Dict] # å®é™…å­˜çš„æ˜¯ NewsBriefing.model_dump()
    
    # [æ–°å¢] å½“å‰é€‰ä¸­çš„è¯¦æƒ…æ¿å— (ä¸ user_preference é•¿æœŸåå¥½åŒºåˆ†å¼€)
    selected_cluster: Optional[str]

    # æ§åˆ¶æµæ ‡å¿—
    intent: Optional[str] # write / read / chat
    force_refresh: Optional[bool] # [æ–°å¢] æ˜¯å¦å¼ºåˆ¶åˆ·æ–°


class RouterDecision(BaseModel):
    """Router å¯¹ç”¨æˆ·æ„å›¾çš„åˆ†æç»“æœ"""
    intent: Literal["write", "read", "chat"] = Field(
        ..., description="ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾"
    )
    category: Optional[str] = Field(
        None, description="æå–å‡ºçš„å…·ä½“é¢†åŸŸå…³é”®è¯ï¼Œå¦‚ 'AI', 'ç§‘æŠ€'"
    )

from tools import fetch_news
from simple_bot import llm_fast, llm_reasoning # Import capability-based LLMs
import json

from langchain_core.prompts import ChatPromptTemplate

def router_node(state: AgentState):
    """è¿›é˜¶ç‰ˆæ„å›¾è¯†åˆ«ï¼šä½¿ç”¨ LLM ç»“æ„åŒ–è¾“å‡º + å®¹é”™å…œåº•"""
    last_message = state["messages"][-1].content
    print(f"ğŸš¦ Router handling message: {last_message}")
    
    # --- æ‹¦æˆªå™¨ 1: è¯¦æƒ…å±•å¼€æŒ‡ä»¤ (æ¥è‡ªå¡ç‰‡æŒ‰é’®) ---
    # åŒ¹é… "å±•å¼€ï¼šXXX" æˆ– "ğŸ‘‰ XXX"
    if "å±•å¼€ï¼š" in last_message or "ğŸ‘‰" in last_message:
        # ç®€å•ç²—æš´æå–ï¼šå–å†’å·æˆ–ç¬¦å·åçš„å†…å®¹ï¼Œå»é™¤æ‹¬å·é‡Œçš„æ•°å­—
        # e.g. "ğŸ‘‰ ç¡¬ä»¶ä¸ç®—åŠ› (8)" -> "ç¡¬ä»¶ä¸ç®—åŠ›"
        import re
        # åŒ¹é… "å±•å¼€ï¼š(.+)" æˆ– "ğŸ‘‰ (.+)"
        match = re.search(r"(?:å±•å¼€ï¼š|ğŸ‘‰\s*)([^\(\)]+)", last_message)
        if match:
            category = match.group(1).strip()
            print(f"ğŸš€ [Router] Intercepted Detail Request: {category}")
            return {"intent": "detail", "selected_cluster": category}
    
    try:
        # å®šä¹‰ System Prompt å¼ºåŒ–æŒ‡ä»¤ (é€‚é… Reasoning æ¨¡å‹)
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ„å›¾è·¯ç”±å™¨ã€‚è¯·åˆ†æç”¨æˆ·çš„è¾“å…¥ï¼Œæå–æ ¸å¿ƒæ„å›¾å’Œå®ä½“ã€‚
        
        è§„åˆ™ï¼š
        1. å¦‚æœç”¨æˆ·æƒ³çœ‹æ–°é—»ã€æ—¥æŠ¥ã€ç®€æŠ¥ -> intent: read
        2. å¦‚æœç”¨æˆ·æƒ³è®¢é˜…ã€å…³æ³¨ã€è¿½è¸ªæŸè¯é¢˜ -> intent: write, category: <è¯é¢˜>
        3. å…¶ä»–æƒ…å†µï¼ˆé—²èŠã€é—®å¥½ã€ä¸æƒ³çœ‹äº†ï¼‰ -> intent: chat
        
        è¾“å‡ºæ ¼å¼ï¼šå¿…é¡»æ˜¯ç¬¦åˆ RouterDecision ç»“æ„çš„ JSONã€‚"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])
        
        # ç»‘å®šå·¥å…· (ä½¿ç”¨ Fast æ¨¡å‹ -> DeepSeek V3)
        print(f"ğŸ¤– User Input: {last_message}")
        structured_llm = llm_fast.with_structured_output(RouterDecision) 
        
        # ç»„åˆ chain
        # chain = prompt | structured_llm
        prompt_message = prompt.invoke({"input": last_message})
        decision = structured_llm.invoke(prompt_message)
        
        print(f"ğŸ‘‰ LLM Decision: {decision.intent}, Category: {decision.category}")
        return {
            "intent": decision.intent, 
            "user_preference": decision.category
        }
    except Exception as e:
        print(f"âš ï¸ Router LLM Error: {e}")
        # å…œåº•ç­–ç•¥ï¼šè¯šå®æŠ¥é”™ï¼Œä¸è¿›è¡ŒçŒœæµ‹
        return {
            "intent": "error",
            "messages": [AIMessage(content=f"âŒ æ„å›¾è¯†åˆ«å¤±è´¥å•¦ã€‚\né”™è¯¯è¯¦æƒ…: {str(e)}")]
        }


from database import upsert_preference, get_preference
from langchain_core.messages import AIMessage

def saver_node(state: AgentState):
    """ä¿å­˜ç”¨æˆ·åå¥½èŠ‚ç‚¹"""
    # 1. ä¼˜å…ˆä½¿ç”¨ Router æå–çš„ç»“æ„åŒ–æ•°æ®
    extracted_category = state.get("user_preference")
    
    # 2. å¦‚æœ Router æ²¡æå‡ºæ¥ï¼Œè¯šå®åœ°è¿”å›é”™è¯¯æç¤ºï¼Œè€Œä¸æ˜¯ççŒœ
    if not extracted_category:
        print("âš ï¸ [Saver] Extraction failed")
        return {"messages": [AIMessage(content="ğŸ¤” æˆ‘çŸ¥é“æ‚¨æƒ³è°ƒæ•´åå¥½ï¼Œä½†æˆ‘æ²¡èƒ½è¯†åˆ«å‡ºå…·ä½“çš„è¯é¢˜ã€‚\n\nè¯·å°è¯•æ›´æ¸…æ™°çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼šâ€œè®¢é˜…AIâ€ã€â€œå…³æ³¨æ¸¸æˆGAMESâ€ã€â€œå…³æ³¨éŸ³ä¹MUSICâ€ã€‚")]}
    
    print(f"ğŸ’¾ [Saver] Saving preference: {extracted_category}")
    
    # 3. å­˜å…¥æ•°æ®åº“
    res = upsert_preference(state["user_id"], extracted_category)
    
    # 4. è¿”å›åŠ¨æ€æ¶ˆæ¯
    return {"messages": [AIMessage(content=f"å·²å…³æ³¨ï¼šã€{extracted_category}ã€‘æ¿å—ï¼Œæ¯æ—¥è‡ªåŠ¨ä¸ºæ‚¨æ¨é€\n\nç‚¹å‡»â€œå½“æ—¥{extracted_category}æ–°é—»â€ï¼Œå³å¯è·å–ä»Šæ—¥åŠ¨æ€ã€‚")]}



def fetcher_node(state: AgentState):
    """
    è´Ÿè´£è·å–æ–°é—»æ•°æ®ï¼š
    1. å…ˆæ£€æŸ¥æ•°æ®åº“ç¼“å­˜ (é™¤é force_refresh=True)
    2. å¦‚æœæ— ç¼“å­˜ï¼Œè°ƒç”¨ Tool æŠ“å– RSS
    """
    print("ğŸ•µï¸ [Fetcher] Node started")
    pref = get_preference(state["user_id"])
    if not pref:
        print("âš ï¸ [Fetcher] No preference found")
        return {
            "user_preference": None, 
            "messages": [AIMessage(content="æ‚¨è¿˜æ²¡æœ‰è®¢é˜…ä»»ä½•å†…å®¹ï¼Œè¯·å‘é€ 'è®¢é˜… AI'ï¼Œ'è®¢é˜… MUSIC'ï¼Œæˆ–è€…'è®¢é˜… GAMES")]
        }
    
    # 1. å°è¯•ä»æ•°æ®åº“è¯»å–ä»Šæ—¥å·²ç”Ÿæˆçš„ç¼“å­˜
    today = date.today().isoformat()
    # æ³¨æ„ï¼šget_cached_news è¿”å› {"content": str, "briefing_data": str/json, "generated_at": str}
    
    # ç­–ç•¥ï¼šå¦‚æœæœ‰ç¼“å­˜ä¸”éå¼ºåˆ¶åˆ·æ–°ï¼Œæˆ‘ä»¬ç›´æ¥è¿”å›ç¼“å­˜
    if not state.get("force_refresh"):
        cached = get_cached_news(pref, today)
        if cached and cached.get("briefing_data"):
            print(f"âœ… [Fetcher] Found cached data for {pref}. generated_at={cached.get('generated_at')}")
            try:
                briefing_json = json.loads(cached["briefing_data"])
                return {
                    "user_preference": pref, 
                    "news_content": None, 
                    "briefing_data": briefing_json,
                    "generated_at": cached.get("generated_at")
                }
            except Exception as e:
                print(f"âš ï¸ [Fetcher] Cache parse failed: {e}")
                pass
    else:
        print(f"ğŸ”„ [Fetcher] Force refresh enabled. Skipping cache check.")

    # 2. æ— ç¼“å­˜æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œæ‰§è¡Œå®æ—¶æŠ“å–
    # 2. æ— ç¼“å­˜æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œæ‰§è¡Œå®æ—¶æŠ“å–
    print(f"ğŸŒ [Fetcher] Fetching news for: {pref}")
    
    news_data = fetch_news(pref)
    
    print(f"âœ… [Fetcher] Got data (length: {len(str(news_data))})")
    return {"user_preference": pref, "news_content": json.dumps(news_data, ensure_ascii=False)}

from messaging import reply_message

from lark_card_builder import build_cover_card

def writer_node(state: AgentState):
    """
    æ ¸å¿ƒå†™ä½œèŠ‚ç‚¹ï¼š
    1. æ¥æ”¶ Fetcher æŠ“å–åˆ°çš„åŸå§‹æ–°é—»æ•°æ®
    2. è°ƒç”¨ Reasoning LLM (DeepSeek R1) è¿›è¡Œæ·±åº¦åˆ†æ
    3. ç”Ÿæˆç»“æ„åŒ–ç®€æŠ¥ (Summary + Clusters)
    4. å°†ç»“æœå­˜å…¥ Stateï¼Œå¹¶æ¸²æŸ“é£ä¹¦å¡ç‰‡
    """
    print("âœï¸ [Writer] Node started")
    
    if state.get("message_id"):
        reply_message(state["message_id"], "âœï¸ AI æ­£åœ¨æ·±åº¦åˆ†ææ–°é—»æ•°æ®ï¼Œç”Ÿæˆäº¤äº’å¼æ—©æŠ¥...")
        
    news_json = state.get("news_content")
    category = state.get("user_preference", "æœªçŸ¥é¢†åŸŸ")
    
    # ç­–ç•¥ 0: å¦‚æœ State ä¸­å·²æœ‰ briefing_data (æ¥è‡ª Cache)ï¼Œç›´æ¥ä½¿ç”¨
    if state.get("briefing_data"):
        try:
            print(f"â© [Writer] Using cached briefing data for {category}")
            # Pydantic è¿˜åŸ
            briefing = NewsBriefing(**state["briefing_data"])
            
            # æ„å»ºå¡ç‰‡ (ä¼ å…¥ generated_at å’Œ category)
            card_content = build_cover_card(briefing, generated_at=state.get("generated_at"), category=category)
            
            return {
                "briefing_data": state["briefing_data"], 
                "messages": [AIMessage(content=card_content)]
            }
        except Exception as e:
            print(f"âš ï¸ [Writer] Failed to reuse cache: {e}, falling back to generation")
            # å¤±è´¥äº†åˆ™ç»§ç»­å¾€ä¸‹æ‰§è¡Œç”Ÿæˆé€»è¾‘
    
    # ç­–ç•¥ 1: å¦‚æœæ²¡æœ‰ News Content (è¿™ä¸åº”è¯¥å‘ç”Ÿï¼ŒFetcher åº”è¯¥å¤„ç†äº†)ï¼ŒæŠ¥é”™
    if not news_json:
        return {"messages": [AIMessage(content="æœªèƒ½è·å–æ–°é—»æ•°æ®")]}
        
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚ç”¨æˆ·çš„è®¢é˜…åå¥½æ˜¯ï¼š{category}ã€‚
    è¯·é˜…è¯»è¾“å…¥çš„æ–°é—» JSON æ•°æ®ï¼Œè¿ç”¨ä½ çš„ä¸“ä¸šæ´å¯ŸåŠ›ï¼Œè¿›è¡Œä»¥ä¸‹å¤„ç†ï¼š

    1. **å»é‡ä¸æ¸…æ´—**ï¼šåˆå¹¶é›·åŒæ–°é—»ï¼Œå‰”é™¤æ— å…³å™ªéŸ³ã€‚
    2. **èšç±»**ï¼šå°†æ–°é—»å½’ç±»ä¸º 3-5 ä¸ªæ ¸å¿ƒæ¿å—ï¼ˆClusterï¼‰ã€‚
    3. **æ‰“åˆ†**ï¼šä¸ºæ¯æ¡æ–°é—»æ‰“åˆ† (1-100)ã€‚
    4. **ç»¼è¿° (Global Summary)**ï¼š
       - **å¿…éœ€**ï¼šé€šè¯»æ‰€æœ‰æ–°é—»ï¼Œå†™ä¸€æ®µ **çŠ€åˆ©ã€å…·ä½“ã€ç›´å‡»è¦å®³** çš„æƒ…æŠ¥ç»¼è¿°ï¼Œé•¿åº¦åœ¨200ä¸­æ–‡å­—ç¬¦å·¦å³ã€‚
       - **ç¦æ­¢**ï¼šå¥—è¯ï¼ˆå¦‚â€œè¡Œä¸šç¨³æ­¥å‘å±•â€ï¼‰ã€åºŸè¯ï¼ˆå¦‚â€œå€¼å¾—å…³æ³¨â€ï¼‰ã€ç¬¼ç»Ÿæè¿°ã€‚
       - **è¦æ±‚**ï¼šå¿…é¡»æåŠå…·ä½“çš„å…¬å¸åã€äº§å“åã€æ ¸å¿ƒäº‰ç«¯æˆ–å…³é”®æ•°æ®ã€‚å®šæ€§ä¸å®šé‡ç»“åˆï¼Œæ–‡å­—å’Œæ•°å­—ç»“åˆï¼Œè¦ç‚¹æ¸…æ™°ï¼Œç›´æ¥å‘Šè¯‰ç”¨æˆ·â€œä»Šå¤©å‘ç”Ÿäº†ä»€ä¹ˆå¤§äº‹ï¼Œæ„å‘³ç€ä»€ä¹ˆâ€ã€‚
    
    è¯·ä¸¥æ ¼è¾“å‡ºç¬¦åˆ NewsBriefing ç»“æ„çš„ JSONã€‚
    **é‡è¦**ï¼š
    1. ç›´æ¥è¾“å‡º JSON å­—ç¬¦ä¸²ï¼Œ**ä¸è¦**åŒ…å« ```json ... ``` ç­‰ Markdown æ ¼å¼ã€‚
    2. JSON æ ¹å¯¹è±¡ç›´æ¥åŒ…å« `global_summary` å’Œ `clusters` å­—æ®µï¼Œ**ä¸è¦**åŒ…è£¹åœ¨ `NewsBriefing` ç­‰æ ¹é”®ä¸‹ã€‚
    3. ä¸è¦åŒ…å«ä»»ä½•æ¨ç†è¿‡ç¨‹æ–‡æœ¬ã€‚"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{news_data}"),
    ])
    
    print("ğŸ§  [Writer] Invoking LLM for Structured Output...")
    # åˆ‡æ¢åˆ° llm_reasoning (Claude 3.5 Sonnet / DeepSeek R1) ä»¥è·å¾—æœ€ä½³å†™ä½œè´¨é‡
    structured_llm = llm_reasoning.with_structured_output(NewsBriefing) 
    chain = prompt | structured_llm
    
    try:
        briefing: NewsBriefing = chain.invoke({"news_data": news_json})
        print(f"âœ… [Writer] Briefing Generated. Clusters: {[c.name for c in briefing.clusters]}")
        
        # 1. æ„å»ºé£ä¹¦äº¤äº’å¡ç‰‡
        card_content = build_cover_card(briefing, category=category)
        
        # 2. è¿”å›ç»“æœ
        # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦æ ‡è®°è¿™æ˜¯ä¸€å¼ å¡ç‰‡ï¼Œè€Œä¸æ˜¯æ™®é€šæ–‡æœ¬
        # ä¸‹æ¸¸å‘é€ç«¯ (lark_service æˆ– messaging) éœ€è¦è¯†åˆ«è¿™ä¸ªæ ‡è®°
        # è¿™é‡Œæˆ‘ä»¬å°† content è®¾ä¸º card jsonï¼Œå¼€å¤´åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Ÿ
        # æˆ–è€…ä½¿ç”¨ additional_kwargs
        
        return {
            "briefing_data": briefing.model_dump(),
            "messages": [AIMessage(content=card_content)] 
        }
    except Exception as e:
        print(f"âŒ [Writer] Analysis Failed: {e}")
        return {"messages": [AIMessage(content=f"ç”Ÿæˆæ—©æŠ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚\nError: {str(e)}")]}


# --- è¯¦æƒ…å±•ç¤ºèŠ‚ç‚¹ ---

from database import get_cached_news # Import at top or inside if circular
from datetime import date, timedelta # [æ–°å¢] timedelta
from database import get_cached_news

# --- è¯¦æƒ…å±•ç¤ºèŠ‚ç‚¹ ---
def detail_node(state: AgentState):
    """
    æ¥æ”¶ç”¨æˆ·é€‰æ‹©çš„æ¿å—å -> ä» State ç¼“å­˜æˆ–æ•°æ®åº“ä¸­æŸ¥æ‰¾æ–°é—» -> æ¸²æŸ“è¯¦æƒ…
    """
    print("ğŸ” [Detail] Node started")
    selected_cluster = state.get("selected_cluster") # ä½¿ç”¨ä¸“é—¨çš„å­—æ®µ
    briefing_dump = state.get("briefing_data")
    
    # ç­–ç•¥ 1: å°è¯•ä» State è·å– (å¦‚æœæ˜¯åŒä¸€ä¼šè¯)
    # ç­–ç•¥ 2: å°è¯•ä»æ•°æ®åº“è·å– (å¦‚æœæ˜¯è·¨ä¼šè¯ç‚¹å‡»)
    if not briefing_dump:
        print(f"âš ï¸ [Detail] State missing briefing_data, searching DB for cluster: {selected_cluster}")
        today = date.today().isoformat()
        categories = ["AI", "GAMES", "MUSIC", "SHORT_DRAMA"] # å·²çŸ¥ç±»åˆ«
        
        for cat in categories:
            cached = get_cached_news(cat, today)
            # get_cached_news è¿”å› {"content": str, "briefing_data": str}
            if cached and cached.get("briefing_data"):
                try:
                    data_json = json.loads(cached["briefing_data"])
                    # æ£€æŸ¥ cluster æ˜¯å¦åœ¨è¿™é‡Œ
                    # ç®€å•æ£€æŸ¥ï¼šç›´æ¥çœ‹ briefing_data å­—ç¬¦ä¸²é‡Œæœ‰æ²¡æœ‰ cluster åå­—ï¼Ÿ
                    # æˆ–è€…è§£æåé€šè¿‡ Pydantic æ£€æŸ¥
                    # ä¸ºæ±‚ç¨³ï¼Œæˆ‘ä»¬å…ˆå°è¯•è§£æ
                    # æ³¨æ„ï¼šNewsBriefing ç»“æ„æ˜¯ global_summary, clusters
                    # è¿™é‡Œæ˜¯ä¸€ä¸ª dict
                    clusters_data = data_json.get("clusters", [])
                    for c in clusters_data:
                        if c.get("name") and (selected_cluster in c["name"] or c["name"] in selected_cluster):
                            print(f"âœ… [Detail] Found cluster in DB category: {cat}")
                            briefing_dump = data_json
                            break
                except Exception as e:
                    print(f"âš ï¸ [Detail] Parse DB cache failed for {cat}: {e}")
            
            if briefing_dump:
                break
    
    if not briefing_dump or not selected_cluster:
        return {"messages": [AIMessage(content=f"âš ï¸ æœªæ‰¾åˆ°æ¿å—ï¼š{selected_cluster}ã€‚\n\næ•°æ®å¯èƒ½å·²æ›´æ–°è¿‡æœŸï¼Œè¯·å‘é€â€œç”Ÿæˆæ—¥æŠ¥â€è·å–æœ€æ–°èµ„è®¯ã€‚")]}
    
    # æ¢å¤ Pydantic å¯¹è±¡
    try:
        briefing = NewsBriefing(**briefing_dump)
    except:
        return {"messages": [AIMessage(content="âš ï¸ æ•°æ®è§£æé”™è¯¯")]}
    
    # æŸ¥æ‰¾å¯¹åº”æ¿å—
    found_cluster = None
    for cluster in briefing.clusters:
        if cluster.name in selected_cluster or selected_cluster in cluster.name:
            found_cluster = cluster
            break
            
    if not found_cluster:
        return {"messages": [AIMessage(content=f"âš ï¸ æœªæ‰¾åˆ°æ¿å—ï¼š{selected_cluster}")]}
        
    # æ¸²æŸ“è¯¦æƒ… (è¿™é‡Œç®€åŒ–ä¸º Markdown æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥åšæˆå¡ç‰‡)
    msg = f"## ğŸ“‚ ä¸“é¢˜è¯¦æƒ…ï¼š{found_cluster.name}\n\n"
    msg += f"_{found_cluster.description}_\n\n"
    for item in found_cluster.items:
        msg += f"### [{item.title}]({item.url})\n"
        msg += f"{item.summary}\n\n"
    
    return {"messages": [AIMessage(content=msg)]}



# --- ç»„è£…å›¾è°± (The Map) ---
from langgraph.graph import StateGraph, END

# 1. æ‹¿å‡ºä¸€å¼ ç©ºç™½åœ°å›¾
workflow = StateGraph(AgentState)

# Chat Node: ä½¿ç”¨ LLM è¿›è¡Œè‡ªç„¶å¯¹è¯
def chat_node(state):
    """èŠå¤©æ¨¡å¼èŠ‚ç‚¹ - è°ƒç”¨ LLM è¿›è¡Œå¤šè½®å¯¹è¯"""
    # state["messages"] å·²åŒ…å«å†å²ä¸Šä¸‹æ–‡ï¼ˆç”± run_agent çš„æ»‘åŠ¨çª—å£æä¾›ï¼‰
    response = llm_fast.invoke(state["messages"])
    return {"messages": [response]}

# 2. åœ¨åœ°å›¾ä¸Šç”»ç«™ç‚¹ (Nodes)
workflow.add_node("router", router_node)
workflow.add_node("saver", saver_node)
workflow.add_node("fetcher", fetcher_node)
workflow.add_node("writer", writer_node)
workflow.add_node("detail", detail_node) # æ–°å¢ Detail èŠ‚ç‚¹
workflow.add_node("chat", chat_node)

# 3. è®¾ç½®èµ·ç‚¹
workflow.set_entry_point("router")

# 4. è®¾ç½®åˆ†å²”è·¯å£
workflow.add_conditional_edges(
    "router",
    lambda x: x["intent"],
    {
        "write": "saver",
        "read": "fetcher",
        "detail": "detail", 
        "chat": "chat",
        "error": END
    }
)

# 5. è®¾ç½®ç»ˆç‚¹
workflow.add_edge("saver", END)
workflow.add_edge("chat", END)
workflow.add_edge("fetcher", "writer")
workflow.add_edge("writer", END)
workflow.add_edge("detail", END) # Detail -> END

# 6. ç¼–è¯‘ï¼ˆå¯ç”¨ Checkpointer ä»¥æŒä¹…åŒ– Stateï¼‰
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)
