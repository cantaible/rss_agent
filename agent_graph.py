from typing import TypedDict, List, Optional, Dict
from langchain_core.messages import BaseMessage
from pydantic import BaseModel, Field
from typing import Literal

# --- Pydantic Data Models (ç”¨äº Writer ç»“æ„åŒ–è¾“å‡º) ---
class NewsItem(BaseModel):
    title: str = Field(..., description="æ–°é—»æ ‡é¢˜")
    summary: str = Field(..., description="æ–°é—»æ‘˜è¦")
    url: str = Field(..., description="åŸæ–‡é“¾æ¥")
    score: int = Field(..., description="é‡è¦æ€§æ‰“åˆ† 1-100")

class NewsCluster(BaseModel):
    name: str = Field(..., description="æ¿å—åç§°ï¼Œå¦‚'ç¡¬ä»¶ä¸ç®—åŠ›'")
    description: str = Field(..., description="æ¿å—ç»¼è¿°")
    items: List[NewsItem] = Field(..., description="è¯¥æ¿å—ä¸‹çš„æ–°é—»åˆ—è¡¨")

class NewsBriefing(BaseModel):
    global_summary: str = Field(..., description="å…¨ç¯‡æ—©æŠ¥çš„å¼€åœºç»¼è¿°")
    top_story_indices: List[int] = Field(None, description="ä»Šæ—¥å¤´æ¡æ–°é—»åœ¨ clusters ä¸­çš„ç´¢å¼•(æš‚ä¸ä½¿ç”¨)")
    # æ³¨æ„ï¼šä¸ºäº†ç®€åŒ–ï¼ŒTop 5 å¯ä»¥åœ¨å±•ç¤ºå±‚é€»è¾‘å¤„ç†ï¼Œæˆ–è€…ç›´æ¥å– clusters é‡Œ score æœ€é«˜çš„
    clusters: List[NewsCluster] = Field(..., description="æ–°é—»åˆ†ç±»æ¿å—")

# --- Agent State ---
class AgentState(TypedDict):
    # æ¶ˆæ¯å†å²
    messages: List[BaseMessage]
    user_id: str
    message_id: Optional[str]
    user_preference: Optional[str]
    news_content: Optional[str] 
    
    # [æ–°å¢] ç»“æ„åŒ–ç®€æŠ¥æ•°æ® (ç”¨äºå¤šè½®å›å¿†)
    briefing_data: Optional[Dict] # å®é™…å­˜çš„æ˜¯ NewsBriefing.model_dump()
    
    # [æ–°å¢] å½“å‰é€‰ä¸­çš„è¯¦æƒ…æ¿å— (ä¸ user_preference é•¿æœŸåå¥½åŒºåˆ†å¼€)
    selected_cluster: Optional[str]

    # æ§åˆ¶æµæ ‡å¿—
    intent: Optional[str] # write / read / chat


class RouterDecision(BaseModel):
    """Router å¯¹ç”¨æˆ·æ„å›¾çš„åˆ†æç»“æœ"""
    intent: Literal["write", "read", "chat"] = Field(
        ..., description="ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾"
    )
    category: Optional[str] = Field(
        None, description="æå–å‡ºçš„å…·ä½“é¢†åŸŸå…³é”®è¯ï¼Œå¦‚ 'AI', 'ç§‘æŠ€'"
    )

from tools import fetch_news
from simple_bot import llm_fast, llm_reasoning # Import capability-based LLMs
import json

from langchain_core.prompts import ChatPromptTemplate

def router_node(state: AgentState):
    """è¿›é˜¶ç‰ˆæ„å›¾è¯†åˆ«ï¼šä½¿ç”¨ LLM ç»“æ„åŒ–è¾“å‡º + å®¹é”™å…œåº•"""
    last_message = state["messages"][-1].content
    print(f"ğŸš¦ Router handling message: {last_message}")
    
    # --- æ‹¦æˆªå™¨ 1: è¯¦æƒ…å±•å¼€æŒ‡ä»¤ (æ¥è‡ªå¡ç‰‡æŒ‰é’®) ---
    # åŒ¹é… "å±•å¼€ï¼šXXX" æˆ– "ğŸ‘‰ XXX"
    if "å±•å¼€ï¼š" in last_message or "ğŸ‘‰" in last_message:
        # ç®€å•ç²—æš´æå–ï¼šå–å†’å·æˆ–ç¬¦å·åçš„å†…å®¹ï¼Œå»é™¤æ‹¬å·é‡Œçš„æ•°å­—
        # e.g. "ğŸ‘‰ ç¡¬ä»¶ä¸ç®—åŠ› (8)" -> "ç¡¬ä»¶ä¸ç®—åŠ›"
        import re
        # åŒ¹é… "å±•å¼€ï¼š(.+)" æˆ– "ğŸ‘‰ (.+)"
        match = re.search(r"(?:å±•å¼€ï¼š|ğŸ‘‰\s*)([^\(\)]+)", last_message)
        if match:
            category = match.group(1).strip()
            print(f"ğŸš€ [Router] Intercepted Detail Request: {category}")
            return {"intent": "detail", "selected_cluster": category}
    
    try:
        # å®šä¹‰ System Prompt å¼ºåŒ–æŒ‡ä»¤ (é€‚é… Reasoning æ¨¡å‹)
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ„å›¾è·¯ç”±å™¨ã€‚è¯·åˆ†æç”¨æˆ·çš„è¾“å…¥ï¼Œæå–æ ¸å¿ƒæ„å›¾å’Œå®ä½“ã€‚
        
        è§„åˆ™ï¼š
        1. å¦‚æœç”¨æˆ·æƒ³çœ‹æ–°é—»ã€æ—¥æŠ¥ã€ç®€æŠ¥ -> intent: read
        2. å¦‚æœç”¨æˆ·æƒ³è®¢é˜…ã€å…³æ³¨ã€è¿½è¸ªæŸè¯é¢˜ -> intent: write, category: <è¯é¢˜>
        3. å…¶ä»–æƒ…å†µï¼ˆé—²èŠã€é—®å¥½ã€ä¸æƒ³çœ‹äº†ï¼‰ -> intent: chat
        
        è¾“å‡ºæ ¼å¼ï¼šå¿…é¡»æ˜¯ç¬¦åˆ RouterDecision ç»“æ„çš„ JSONã€‚"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])
        
        # ç»‘å®šå·¥å…· (ä½¿ç”¨ Fast æ¨¡å‹ -> DeepSeek V3)
        print(f"ğŸ¤– User Input: {last_message}")
        structured_llm = llm_fast.with_structured_output(RouterDecision) 
        
        # ç»„åˆ chain
        # chain = prompt | structured_llm
        prompt_message = prompt.invoke({"input": last_message})
        decision = structured_llm.invoke(prompt_message)
        
        print(f"ğŸ‘‰ LLM Decision: {decision.intent}, Category: {decision.category}")
        return {
            "intent": decision.intent, 
            "user_preference": decision.category
        }
    except Exception as e:
        print(f"âš ï¸ Router LLM Error: {e}")
        # å…œåº•ç­–ç•¥ï¼šè¯šå®æŠ¥é”™ï¼Œä¸è¿›è¡ŒçŒœæµ‹
        return {
            "intent": "error",
            "messages": [AIMessage(content=f"âŒ æ„å›¾è¯†åˆ«å¤±è´¥å•¦ã€‚\né”™è¯¯è¯¦æƒ…: {str(e)}")]
        }


from database import upsert_preference, get_preference
from langchain_core.messages import AIMessage

def saver_node(state: AgentState):
    """ä¿å­˜ç”¨æˆ·åå¥½èŠ‚ç‚¹"""
    # 1. ä¼˜å…ˆä½¿ç”¨ Router æå–çš„ç»“æ„åŒ–æ•°æ®
    extracted_category = state.get("user_preference")
    
    # 2. å¦‚æœ Router æ²¡æå‡ºæ¥ï¼Œè¯šå®åœ°è¿”å›é”™è¯¯æç¤ºï¼Œè€Œä¸æ˜¯ççŒœ
    if not extracted_category:
        print("âš ï¸ [Saver] Extraction failed")
        return {"messages": [AIMessage(content="ğŸ¤” æˆ‘çŸ¥é“æ‚¨æƒ³è°ƒæ•´åå¥½ï¼Œä½†æˆ‘æ²¡èƒ½è¯†åˆ«å‡ºå…·ä½“çš„è¯é¢˜ã€‚\n\nè¯·å°è¯•æ›´æ¸…æ™°çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼šâ€œè®¢é˜…AIâ€ã€â€œå…³æ³¨æ¸¸æˆGAMESâ€ã€â€œå…³æ³¨éŸ³ä¹MUSICâ€ã€‚")]}
    
    print(f"ğŸ’¾ [Saver] Saving preference: {extracted_category}")
    
    # 3. å­˜å…¥æ•°æ®åº“
    res = upsert_preference(state["user_id"], extracted_category)
    
    # 4. è¿”å›åŠ¨æ€æ¶ˆæ¯
    return {"messages": [AIMessage(content=f"å·²ä¸ºæ‚¨å…³æ³¨æ¿å—ï¼šã€{extracted_category}ã€‘\n\nå‘é€â€œçœ‹æ–°é—»â€å³å¯è·å–è¯¥æ¿å—åŠ¨æ€ã€‚")]}



def fetcher_node(state: AgentState):
    """è¯»å–åå¥½ -> æŠ“å–æ–°é—»"""
    print("ğŸ•µï¸ [Fetcher] Node started")
    pref = get_preference(state["user_id"])
    if not pref:
        print("âš ï¸ [Fetcher] No preference found")
        return {
            "user_preference": None, 
            "messages": [AIMessage(content="æ‚¨è¿˜æ²¡æœ‰è®¢é˜…ä»»ä½•å†…å®¹ï¼Œè¯·å‘é€ 'è®¢é˜… AI'")]
        }
    
    print(f"ğŸŒ [Fetcher] Fetching news for: {pref}")
    news_data = fetch_news(pref)
    print(f"âœ… [Fetcher] Got data (length: {len(str(news_data))})")
    return {"user_preference": pref, "news_content": json.dumps(news_data, ensure_ascii=False)}

from messaging import reply_message

from lark_card_builder import build_cover_card

def writer_node(state: AgentState):
    """
    æ ¸å¿ƒå†™ä½œèŠ‚ç‚¹ï¼š
    1. æ¥æ”¶ Fetcher æŠ“å–åˆ°çš„åŸå§‹æ–°é—»æ•°æ®
    2. è°ƒç”¨ Reasoning LLM (DeepSeek R1) è¿›è¡Œæ·±åº¦åˆ†æ
    3. ç”Ÿæˆç»“æ„åŒ–ç®€æŠ¥ (Summary + Clusters)
    4. å°†ç»“æœå­˜å…¥ Stateï¼Œå¹¶æ¸²æŸ“é£ä¹¦å¡ç‰‡
    """
    print("âœï¸ [Writer] Node started")
    
    if state.get("message_id"):
        reply_message(state["message_id"], "âœï¸ AI æ­£åœ¨æ·±åº¦åˆ†ææ–°é—»æ•°æ®ï¼Œç”Ÿæˆäº¤äº’å¼æ—©æŠ¥...")
        
    news_json = state.get("news_content")
    category = state.get("user_preference", "æœªçŸ¥é¢†åŸŸ")
    
    if not news_json:
        print("âŒ [Writer] No news content")
        return {"messages": [AIMessage(content="æœªè·å–åˆ°æ–°é—»æ•°æ®ã€‚")]}
        
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆã€‚ç”¨æˆ·çš„è®¢é˜…åå¥½æ˜¯ï¼š{category}ã€‚
    è¯·é˜…è¯»è¾“å…¥çš„æ–°é—» JSON æ•°æ®ï¼Œè¿›è¡Œä»¥ä¸‹å¤„ç†ï¼š
    1. **å»é‡**ï¼šåˆå¹¶å†…å®¹é›·åŒçš„æ–°é—»ã€‚
    2. **èšç±»**ï¼šå°†æ–°é—»å½’ç±»ä¸º 3-5 ä¸ªæ ¸å¿ƒæ¿å—ï¼ˆClusterï¼‰ï¼Œå¦‚ "ç¡¬ä»¶"ã€"ç›‘ç®¡"ã€"åº”ç”¨"ã€‚
    3. **æ‰“åˆ†**ï¼šä¸ºæ¯æ¡æ–°é—»æ‰“åˆ† (1-100)ã€‚
    4. **Top 5**ï¼šæŒ‘é€‰å‡ºæœ€é‡è¦çš„ 5 æ¡æ–°é—»ã€‚
    5. **ç»¼è¿°**ï¼šå†™ä¸€æ®µå…¨å±€çš„è¡Œä¸šè¶‹åŠ¿åˆ†æã€‚
    
    è¯·ä¸¥æ ¼è¾“å‡ºç¬¦åˆ NewsBriefing ç»“æ„çš„ JSONã€‚
    **é‡è¦**ï¼š
    1. ç›´æ¥è¾“å‡º JSON å­—ç¬¦ä¸²ï¼Œ**ä¸è¦**åŒ…å« ```json ... ``` ç­‰ Markdown æ ¼å¼ã€‚
    2. JSON æ ¹å¯¹è±¡ç›´æ¥åŒ…å« `global_summary` å’Œ `clusters` å­—æ®µï¼Œ**ä¸è¦**åŒ…è£¹åœ¨ `NewsBriefing` ç­‰æ ¹é”®ä¸‹ã€‚
    3. ä¸è¦åŒ…å«ä»»ä½•æ¨ç†è¿‡ç¨‹æ–‡æœ¬ã€‚"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{news_data}"),
    ])
    
    print("ğŸ§  [Writer] Invoking LLM for Structured Output...")
    # åˆ‡æ¢å› llm_fast (Gemini)ï¼Œå› ä¸ºå®ƒåœ¨ JSON æ ¼å¼éµå¾ªä¸Šæ›´ç¨³å®š
    structured_llm = llm_fast.with_structured_output(NewsBriefing) 
    chain = prompt | structured_llm
    
    try:
        briefing: NewsBriefing = chain.invoke({"news_data": news_json})
        print(f"âœ… [Writer] Briefing Generated. Clusters: {[c.name for c in briefing.clusters]}")
        
        # 1. æ„å»ºé£ä¹¦äº¤äº’å¡ç‰‡
        card_content = build_cover_card(briefing)
        
        # 2. è¿”å›ç»“æœ
        # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦æ ‡è®°è¿™æ˜¯ä¸€å¼ å¡ç‰‡ï¼Œè€Œä¸æ˜¯æ™®é€šæ–‡æœ¬
        # ä¸‹æ¸¸å‘é€ç«¯ (lark_service æˆ– messaging) éœ€è¦è¯†åˆ«è¿™ä¸ªæ ‡è®°
        # è¿™é‡Œæˆ‘ä»¬å°† content è®¾ä¸º card jsonï¼Œå¼€å¤´åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Ÿ
        # æˆ–è€…ä½¿ç”¨ additional_kwargs
        
        return {
            "briefing_data": briefing.model_dump(),
            "messages": [AIMessage(content=card_content)] 
        }
    except Exception as e:
        print(f"âŒ [Writer] Analysis Failed: {e}")
        return {"messages": [AIMessage(content=f"ç”Ÿæˆæ—©æŠ¥å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚\nError: {str(e)}")]}


# --- è¯¦æƒ…å±•ç¤ºèŠ‚ç‚¹ ---
def detail_node(state: AgentState):
    """
    æ¥æ”¶ç”¨æˆ·é€‰æ‹©çš„æ¿å—å -> ä» State ç¼“å­˜ä¸­æŸ¥æ‰¾æ–°é—» -> æ¸²æŸ“è¯¦æƒ…
    """
    print("ğŸ” [Detail] Node started")
    selected_cluster = state.get("selected_cluster") # ä½¿ç”¨ä¸“é—¨çš„å­—æ®µ
    briefing_dump = state.get("briefing_data")
    
    if not briefing_dump or not selected_cluster:
        return {"messages": [AIMessage(content="âš ï¸ æŠ±æ­‰ï¼Œæ—©æŠ¥æ•°æ®å·²è¿‡æœŸæˆ–æœªæ‰¾åˆ°ã€‚è¯·é‡æ–°å‘é€ 'çœ‹æ–°é—»' è·å–æœ€æ–°æ—©æŠ¥ã€‚")]}
    
    # æ¢å¤ Pydantic å¯¹è±¡
    try:
        briefing = NewsBriefing(**briefing_dump)
    except:
        return {"messages": [AIMessage(content="âš ï¸ æ•°æ®è§£æé”™è¯¯")]}
    
    # æŸ¥æ‰¾å¯¹åº”æ¿å—
    found_cluster = None
    for cluster in briefing.clusters:
        if cluster.name in selected_cluster or selected_cluster in cluster.name:
            found_cluster = cluster
            break
            
    if not found_cluster:
        return {"messages": [AIMessage(content=f"âš ï¸ æœªæ‰¾åˆ°æ¿å—ï¼š{selected_cluster}")]}
        
    # æ¸²æŸ“è¯¦æƒ… (è¿™é‡Œç®€åŒ–ä¸º Markdown æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥åšæˆå¡ç‰‡)
    msg = f"## ğŸ“‚ ä¸“é¢˜è¯¦æƒ…ï¼š{found_cluster.name}\n\n"
    msg += f"_{found_cluster.description}_\n\n"
    for item in found_cluster.items:
        msg += f"### [{item.title}]({item.url})\n"
        msg += f"{item.summary}\n\n"
    
    return {"messages": [AIMessage(content=msg)]}


# --- ç»„è£…å›¾è°± (The Map) ---
from langgraph.graph import StateGraph, END

# 1. æ‹¿å‡ºä¸€å¼ ç©ºç™½åœ°å›¾
workflow = StateGraph(AgentState)

# Chat Node: ä½¿ç”¨ LLM è¿›è¡Œè‡ªç„¶å¯¹è¯
def chat_node(state):
    """èŠå¤©æ¨¡å¼èŠ‚ç‚¹ - è°ƒç”¨ LLM è¿›è¡Œå¤šè½®å¯¹è¯"""
    # state["messages"] å·²åŒ…å«å†å²ä¸Šä¸‹æ–‡ï¼ˆç”± run_agent çš„æ»‘åŠ¨çª—å£æä¾›ï¼‰
    response = llm_fast.invoke(state["messages"])
    return {"messages": [response]}

# 2. åœ¨åœ°å›¾ä¸Šç”»ç«™ç‚¹ (Nodes)
workflow.add_node("router", router_node)
workflow.add_node("saver", saver_node)
workflow.add_node("fetcher", fetcher_node)
workflow.add_node("writer", writer_node)
workflow.add_node("detail", detail_node) # æ–°å¢ Detail èŠ‚ç‚¹
workflow.add_node("chat", chat_node)

# 3. è®¾ç½®èµ·ç‚¹
workflow.set_entry_point("router")

# 4. è®¾ç½®åˆ†å²”è·¯å£
workflow.add_conditional_edges(
    "router",
    lambda x: x["intent"],
    {
        "write": "saver",
        "read": "fetcher",
        "detail": "detail", 
        "chat": "chat",
        "error": END
    }
)

# 5. è®¾ç½®ç»ˆç‚¹
workflow.add_edge("saver", END)
workflow.add_edge("chat", END)
workflow.add_edge("fetcher", "writer")
workflow.add_edge("writer", END)
workflow.add_edge("detail", END) # Detail -> END

# 6. ç¼–è¯‘ï¼ˆå¯ç”¨ Checkpointer ä»¥æŒä¹…åŒ– Stateï¼‰
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)
